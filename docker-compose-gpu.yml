version: "3.6"
services:
  intelec-backend:
    image: intelecai/automl-server:latest-gpu
    volumes:
      - type: volume
        source: user-home
        target: /home
      - type: volume
        source: training
        target: /training
      - type: volume
        source: deployed-models
        target: /deployed-models
      - type: volume
        source: root
        target: /root
      - type: tmpfs
        target: /dev/shm
        tmpfs:
          size: 2000000000
    deploy:
      restart_policy:
        condition: on-failure
    env_file:
      - config/automl.env
    environment:
      - INFERENCE_SERVER_PUBLIC_URL=http://{this_host}:7701
    networks:
      - webnet
      - dbnet

  intelec-inference:
    image: intelecai/inference-server
    volumes:
      - type: volume
        source: deployed-models
        target: /models
        read_only: true
    environment:
      - MODELS_IN_MEMORY=2
    deploy:
      restart_policy:
        condition: on-failure
    networks:
      - webnet

  db:
    image: mysql:5.7.24
    env_file:
      - config/db/mysql_root_password.env
    environment:
      MYSQL_DATABASE: intelec_ai
    volumes:
      - type: volume
        source: db-data
        target: /var/lib/mysql
      - type: bind
        source: config/db
        target: /docker-entrypoint-initdb.d
        read_only: true
    deploy:
      restart_policy:
        condition: on-failure
    networks:
      - dbnet

  nginx:
    image: nginx:1.15-alpine    
    volumes:
      - type: bind
        source: config/nginx
        target: /etc/nginx/conf.d
        read_only: true
    deploy:
      restart_policy:
        condition: on-failure
    ports:
      - "7700:80"
      - "7701:81"
    networks: 
      - webnet
      
volumes:
  db-data:
  user-home:
  training:
  deployed-models:
  root:

networks:
  webnet:
  dbnet: